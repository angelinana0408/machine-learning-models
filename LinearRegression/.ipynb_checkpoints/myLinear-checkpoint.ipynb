{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Closed Form Without Normalization\n",
      "Beta: \n",
      "[-0.0862246   0.05340575  0.65803045  0.41731923 -0.01772481  0.30069864\n",
      "  1.02871152  0.48383363  0.26685697  0.04573456  0.31944742  1.14776959\n",
      "  0.29366213  0.41491543  0.85180482 -0.05950309  0.47235562  0.46198106\n",
      "  0.00497427  0.0205398   0.41310473  0.98508025  0.15573467  0.8618602\n",
      "  0.41974331 -0.06893699  0.33317496  0.27766637 -0.04184791 -0.23599504\n",
      "  0.15020297  0.37745027  0.80256455  0.16053288  0.2744667   0.63461071\n",
      "  0.74135259  0.56079776  0.94058723 -0.0432542   0.80803615  0.93967722\n",
      "  0.12225161 -0.19933624  0.09398732  0.11412993  0.35479619  0.78582876\n",
      "  0.38900433  0.11804526  0.67618837  0.70377377  0.05526258 -0.24919095\n",
      "  0.87339793 -0.01381723  0.83138416  0.90569236  0.39980648  0.25235308\n",
      "  0.69692397 -0.00949757  0.17676599  0.45822485  0.02743899  1.16718165\n",
      "  0.04176352  1.01993881  0.56015024 -0.29761224  0.3177761   0.55781578\n",
      "  1.1376088   0.55190283  0.4099807   0.91987238  1.34076835  0.53297825\n",
      "  0.63648277  0.22140583  0.21469531 -0.00609269  0.82898663  0.46891532\n",
      " -0.25571565  0.1972989   1.38639797  0.87219453  0.65782257  0.54983464\n",
      "  1.11698567  0.94267463  0.79030138  0.30055848  0.53288973  0.22873689\n",
      "  0.86702876  0.98591924  0.08132528  0.30834368  0.70121488]\n",
      "MSE:  4.39609786082\n",
      "------------------------------------------------\n",
      "Batch Gradient Without Normalization\n",
      "Beta: \n",
      "[ 0.31621782  0.19373823  0.77210088  0.72102115  0.95332946  0.80650495\n",
      "  0.12069204  0.31294308  0.92697283  0.83086489  0.34385133  0.01829305\n",
      "  0.50317166  0.02176839  0.7023698  -0.00463561  0.62642681  0.60382122\n",
      "  0.89619694  0.46795782  0.31574545  0.20522854  0.70158184  0.81519844\n",
      "  0.55645861  0.20091429  0.52167445  0.70772175  0.57122709  0.09402624\n",
      "  0.28103103  0.71288147  0.92718275  0.41635037  0.29666731  0.19822211\n",
      "  0.64975947  0.21317854  0.17211591  0.35840691  0.13368765  0.44341939\n",
      "  0.35876086 -0.02281754  0.21497991  0.76752452  0.71653877  0.23524312\n",
      "  0.70274878  0.09887135  0.24984597  0.22498496  0.47982048  0.02281689\n",
      "  0.6979966   0.81093942  0.1854445   0.20556874  0.71837004  0.78581394\n",
      "  0.58318433  0.5323156   0.68097684  0.21085753  0.20898403  0.07463247\n",
      "  0.42748551  0.0159705   0.71486125  0.19440795  0.22261818  0.23294205\n",
      "  0.65617287  0.3169698   0.24375817  0.08406445  0.05696466  0.26038516\n",
      "  0.36190552  0.14789575  0.39381624  0.37577658  0.75369335  0.25679111\n",
      "  0.74122579  0.24772476  0.06678714  0.13184062  0.46205997  0.51556858\n",
      "  0.24384223  0.82038939  0.41035986  0.33573123  0.5356275   0.47478178\n",
      "  0.43746092  0.85979217  0.09178742  0.92337856  0.72385529]\n",
      "MSE:  5.63968580592\n",
      "------------------------------------------------\n",
      "Stochastic Gradient Without Normalization\n",
      "Beta: \n",
      "[ 0.47074431  0.1054565   0.56967775  0.16587951  0.77068513  0.92378055\n",
      "  0.79436483  0.6392072   0.61635072  0.55277314  0.2247171   0.42374099\n",
      " -0.03565456  0.61767666  0.26527245  0.78230899  0.18569883  0.71753431\n",
      "  0.67686737  0.3157166   0.11553687  0.56150067  0.86076538  0.3907126\n",
      "  0.85975354  0.11507376  0.19184358  0.50649768  0.47466833  0.88532506\n",
      "  0.44937207  0.82044892  0.79823802  0.64885919  0.73176707  0.17212658\n",
      "  0.9263898   0.1889819   0.87101053  0.2028571   0.61673033  0.6045872\n",
      "  0.46653202  0.69974402  0.30576809  0.62771085  0.00500061  0.07501294\n",
      "  0.32871085  0.11527163  0.77963518  0.77928159  0.88096709  0.04148452\n",
      "  0.49790822  0.39742728  0.65167534  0.07862029  0.50342522  0.63617764\n",
      " -0.01146139  0.17043263  0.82953149  0.24317658  0.00809448  0.42651903\n",
      "  0.65061772  0.14102426  0.26463962  0.05573711  0.15678366  0.15981553\n",
      "  0.23721657  0.40329993  0.12286005  0.52993081  0.51032582  0.91591166\n",
      "  0.29049529  0.26794507  0.60728549  0.78387513  0.53738897  0.45510696\n",
      "  0.12611734  0.03136304  0.91187459  0.1942846   0.15706953  0.03620456\n",
      "  0.70330234  0.28838778  0.02737053  0.80927649  0.47715793  0.24170865\n",
      "  0.03663613  0.08014239  0.33320859  0.55389253  0.65471737]\n",
      "MSE:  5.45410770608\n",
      "------------------------------------------------\n",
      "Closed Form With Normalization\n",
      "Beta: \n",
      "[ 1.92750214  0.04877801  0.6749943   0.38659919  0.08853244  0.36930588\n",
      "  1.02920127  0.54124587  0.3367702  -0.14825513  0.37845404  1.05779107\n",
      "  0.36138445  0.35713653  0.70479468 -0.2369577   0.51927667  0.3071519\n",
      " -0.02251202 -0.10426571  0.25100856  1.04901146  0.20048535  0.88941454\n",
      "  0.2837188  -0.17112352  0.23465302  0.30036408 -0.24855857 -0.38521932\n",
      "  0.16524064  0.45093337  0.5825216   0.11089637  0.17701852  0.66258228\n",
      "  0.74847395  0.55625091  0.94480328 -0.19447063  0.87764977  0.80572902\n",
      "  0.06373803 -0.25362551  0.14783682  0.12874552  0.18352428  0.61615128\n",
      "  0.36126407 -0.03869918  0.70599934  0.77202807  0.02619275 -0.40395855\n",
      "  0.82923481 -0.18352956  0.82166889  1.02122038  0.42673854  0.2863236\n",
      "  0.70016176 -0.06564746  0.08639955  0.28417641 -0.10116979  1.25655356\n",
      " -0.13766681  0.86827153  0.46964788 -0.19552161  0.17303998  0.43560675\n",
      "  1.08307562  0.60554749  0.44432571  1.05719294  1.24486242  0.54226394\n",
      "  0.41185958  0.33378353  0.2761711  -0.10198151  0.68528726  0.36606672\n",
      " -0.25232408  0.25513358  1.36750252  0.8264247   0.61088344  0.51030489\n",
      "  1.01364978  1.01085414  0.84551553  0.3534138   0.58473755  0.29245479\n",
      "  0.74042801  0.98200812 -0.09087576  0.20413793  0.66342553]\n",
      "MSE:  4.89455894368\n",
      "------------------------------------------------\n",
      "Batch Gradient With Normalization\n",
      "Beta: \n",
      "[-0.04138982 -0.09090463  0.11758284  0.3619585   0.37046442  0.84558741\n",
      "  0.57756318  0.39983435  0.80344263  0.06089654  0.25256538  0.46855077\n",
      "  0.48655971  0.36657545  0.30702887  0.59007346  0.1560018   0.85391498\n",
      " -0.03909877  0.18965244  0.29615092  0.89273797  0.28363118  0.39845674\n",
      "  0.22765171 -0.02713526  0.25208651  0.41695111  0.33459696  0.60683495\n",
      "  0.35840654  0.13327071  0.72251843  0.3357416   0.13405023  0.76451269\n",
      "  0.72906698  0.89210001  0.62452694  0.48680289  0.43787262  0.30921659\n",
      "  0.74567023  0.60660023  0.87932912  0.74118773  0.80300813  0.8536391\n",
      "  0.58024637  0.44773848  0.52959266  0.43667691  0.05396317  0.81246558\n",
      "  0.87998692  0.48826145  0.38612265  0.46202367 -0.00360895  0.93380346\n",
      "  0.01156207  0.5948422   0.72602649  0.12031168  0.80415379  0.54503807\n",
      "  0.92747456  0.24834472  0.67627238  0.92237207 -0.02372719  0.54260327\n",
      "  0.35364845  0.44979078  0.71867337  0.67359084 -0.03572331  0.371179\n",
      "  0.2451603   0.46049761  0.67441429  0.14679227  0.720856    0.47133269\n",
      "  0.14804858  0.63165946  0.87316158  0.55281657  0.29778333  0.80469668\n",
      "  0.05659298  0.76619358  0.65668285 -0.03874487 -0.04079372  0.87340532\n",
      "  0.51094182  0.63770345  0.48546091  0.68642474  0.04430042]\n",
      "MSE:  6.44473103851\n",
      "------------------------------------------------\n",
      "Stochastic Gradient With Normalization\n",
      "Beta: \n",
      "[ 0.63243923 -0.13401251  0.89017772  0.19490762  0.61292182  0.64541219\n",
      "  0.66362645  0.0147719   0.78012337  0.50783529  0.68765425  0.8594224\n",
      "  0.38234135  0.23152945  0.59570241  0.23041119  0.32852687  0.7112022\n",
      "  0.38759974  0.13815102  0.70461878  0.6955516   0.73735376  0.66693804\n",
      "  0.67352644  0.73076546  0.8271555   0.66711002  0.82300437  0.8839374\n",
      " -0.02416985 -0.05413068  0.56339438  0.90576873  0.6447431   0.46348441\n",
      "  0.02495631 -0.03200845  0.51304241  0.2708399   0.42133997  0.66871762\n",
      "  0.57606484  0.31543926  0.09870542  0.51997102  0.05750449  0.8458482\n",
      "  0.06631694  0.54816582  0.01836376  0.43917483  0.68734217  0.22698783\n",
      "  0.75007424 -0.0449783   0.35612447  0.91623925  0.48471079  0.31395281\n",
      "  0.08829634  0.31086194  0.78287869  0.62191784  0.04746114  0.52376429\n",
      "  0.81192323  0.49911175  0.80365378  0.21436709  0.58472581  0.040778\n",
      "  0.6736155   0.3638458   0.7266725   0.53160198  0.41027188  0.21342953\n",
      "  0.13526155  0.45254869 -0.05997356  0.66997774  0.48487819  0.67346208\n",
      "  0.51103767  0.83842728  0.82423107  0.50831098  0.62699382  0.3396732\n",
      "  0.26606024  0.00936043  0.65295546  0.03428596  0.31609022  0.15803614\n",
      "  0.71055413  0.55063691  0.11747433  0.38892144  0.63077479]\n",
      "MSE:  6.49151564516\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Reads the data from CSV files, converts it into Dataframe and returns x and y dataframes\n",
    "def getDataframe(filePath):\n",
    "    dataframe = pd.read_csv(filePath)\n",
    "    y = dataframe['y']\n",
    "    x = dataframe.drop('y', axis=1)\n",
    "    return x, y\n",
    "\n",
    "# Applies z-score normalization to the dataframe and returns a normalized dataframe\n",
    "def applyZScore(dataframe): #dataframe size(1000,100), without 1 col at the first col\n",
    "    normalized_dataframe = dataframe\n",
    "    ########## Please Fill Missing Lines Here ##########\n",
    "    data_x = dataframe.values\n",
    "    featureNum = data_x.shape[1]\n",
    "    dataNum = data_x.shape[0]\n",
    "    #print 'featureNum: ', featureNum\n",
    "    #print 'dataNum: ', dataNum\n",
    "    for featureIndex in range(0, featureNum):\n",
    "        dataSum = 0\n",
    "        sumTemp = 0\n",
    "        #calculate miu(mean) for this feature\n",
    "        for dataIndex in range(0, dataNum):\n",
    "            #sum all the data for the (featureIndex)th feature \n",
    "            dataSum = dataSum + data_x[dataIndex][featureIndex]\n",
    "        meanValue = dataSum / dataNum\n",
    "        #calculate standard deviation for this feature\n",
    "        for dataIndexSD in range(0, dataNum):\n",
    "            sumTemp = sumTemp + (data_x[dataIndexSD][featureIndex] - meanValue) * (data_x[dataIndexSD][featureIndex] - meanValue)\n",
    "        SD = np.sqrt(sumTemp / (dataNum - 1))\n",
    "        #put standardalized value back to the dataframe\n",
    "        for dataIndexPB in range(0, dataNum):\n",
    "            standardalized_value = (data_x[dataIndexPB][featureIndex] - meanValue) / SD\n",
    "            normalized_dataframe.set_value(0, 'x'+str(featureIndex+1), standardalized_value) #the 0 is number 0 not str 0\n",
    "            #print 'dataIndexPB: ', dataIndexPB\n",
    "            #print 'featureIndex: ', 'x'+str(featureIndex+1)\n",
    "            #print 'normalized_dataframe size: ', normalized_dataframe.shape\n",
    "    \n",
    "    return normalized_dataframe\n",
    "\n",
    "# train_x and train_y are numpy arrays\n",
    "# function returns value of beta calculated using (0) the formula beta = (X^T*X)^ -1)*(X^T*Y)\n",
    "def getBeta(train_x, train_y): #train_x size(1000,101), with 1 col at the first col from now on\n",
    "    #print 'train_x shape'\n",
    "    #print train_x.shape\n",
    "    beta = np.zeros(train_x.shape[1])\n",
    "    ########## Please Fill Missing Lines Here ##########\n",
    "    beta = np.linalg.inv(train_x.transpose().dot(train_x)).dot((train_x.transpose().dot(train_y)))\n",
    "    return beta\n",
    "    \n",
    "# train_x and train_y are numpy arrays\n",
    "# alpha (learning rate) is a scalar\n",
    "# function returns value of beta calculated using (1) batch gradient descent\n",
    "def getBetaBatchGradient(train_x, train_y, alpha):\n",
    "    beta = np.random.rand(train_x.shape[1])\n",
    "    ########## Please Fill Missing Lines Here ##########\n",
    "    #flag = 1\n",
    "    #count = 0\n",
    "    rowNum = train_x.shape[0]\n",
    "    derivative = np.copy(beta)\n",
    "    numIterations = 1000\n",
    "    middle = 0 #is a value\n",
    "    #MSE = 100000\n",
    "    #MSE = np.iinfo(np.int32).max\n",
    "    #while flag != 0:\n",
    "    for iterTimes in range(0, numIterations):\n",
    "        derivative.fill(0)\n",
    "        MSE_middle = 0 #is a value\n",
    "        for row_index in range(0, rowNum):     \n",
    "            row_temp = train_x[row_index,:]\n",
    "            middleStep = np.subtract(row_temp.transpose().dot(beta), train_y[row_index])\n",
    "            MSE_middle = MSE_middle + middleStep*middleStep\n",
    "            derivative = np.add(derivative , row_temp.dot(middleStep))\n",
    "        beta = np.subtract(beta , np.multiply(alpha, derivative)) \n",
    "        #total square error\n",
    "        #MSE_temp = (MSE_middle*MSE_middle)/rowNum   \n",
    "        #if MSE_temp > MSE:\n",
    "            #break \n",
    "        #else:\n",
    "            #MSE = MSE_temp\n",
    "            \n",
    "        #if (beta_temp == beta).all():\n",
    "            #flag = 0\n",
    "            #break\n",
    "        #else:\n",
    "            #flag = 1\n",
    "            #beta = np.copy(beta_temp)\n",
    "        #count = count + 1\n",
    "        #print 'count'\n",
    "        #print count\n",
    "    return beta\n",
    "    \n",
    "# train_x and train_y are numpy arrays\n",
    "# alpha (learning rate) is a scalar\n",
    "# function returns value of beta calculated using (2) stochastic gradient descent\n",
    "def getBetaStochasticGradient(train_x, train_y, alpha):\n",
    "    beta = np.random.rand(train_x.shape[1])\n",
    "    #beta = getBeta(train_x, train_y)\n",
    "    ########## Please Fill Missing Lines Here ##########\n",
    "    rowNum = train_x.shape[0]\n",
    "    numIterations = 1000\n",
    "    for iterTimes in range(0, numIterations):\n",
    "        for row_index in range(0, rowNum): \n",
    "            row_temp = train_x[row_index,:]\n",
    "            derivative = np.multiply((np.subtract(train_y[row_index], row_temp.transpose().dot(beta) )),(row_temp))\n",
    "            beta = np.add(beta, np.multiply(alpha, derivative))\n",
    "    return beta\n",
    "\n",
    "# predicted_y and test_y are the predicted and actual y values respectively as numpy arrays\n",
    "# function prints the mean squared error value for the test dataset\n",
    "def compute_mse(predicted_y, test_y):\n",
    "    mse = 100.0\n",
    "    ########## Please Fill Missing Lines Here ##########\n",
    "    dataNum = test_y.shape[0]\n",
    "    SESum = 0\n",
    "    for i in range(0,dataNum ):\n",
    "        SESum = SESum + (predicted_y[i] - test_y[i])*(predicted_y[i] - test_y[i])\n",
    "    mse = SESum / dataNum\n",
    "    print 'MSE: ', mse\n",
    "    \n",
    "# Linear Regression implementation\n",
    "class LinearRegression(object):\n",
    "    # Initializes by reading data, setting hyper-parameters, and forming linear model\n",
    "    # Forms a linear model (learns the parameter) according to type of beta (0 - closed form, 1 - batch gradient, 2 - stochastic gradient)\n",
    "    # Performs z-score normalization if z_score is 1\n",
    "    def __init__(self, beta_type, z_score = 0):\n",
    "        #make alpha smaller\n",
    "        self.alpha = 0.001/100\n",
    "        self.beta_type = beta_type\n",
    "        self.z_score = z_score\n",
    "\n",
    "        self.train_x, self.train_y = getDataframe('linear-regression-train.csv')\n",
    "        self.test_x, self.test_y = getDataframe('linear-regression-test.csv')\n",
    "        \n",
    "        if(z_score == 1):\n",
    "            self.train_x = applyZScore(self.train_x)\n",
    "            self.test_x = applyZScore(self.test_x)\n",
    "        \n",
    "        # Prepend columns of 1 for beta 0, train_x's col plus 1 from now on\n",
    "        self.train_x.insert(0, 'offset', 1) \n",
    "        self.test_x.insert(0, 'offset', 1)\n",
    "        \n",
    "        self.linearModel()\n",
    "    \n",
    "    # Gets the beta according to input\n",
    "    def linearModel(self):\n",
    "        if(self.beta_type == 0):\n",
    "            self.beta = getBeta(self.train_x.values, self.train_y.values)\n",
    "            \n",
    "            print 'Beta: '\n",
    "            print self.beta\n",
    "        elif(self.beta_type == 1):\n",
    "            self.beta = getBetaBatchGradient(self.train_x.values, self.train_y.values, self.alpha)\n",
    "            print 'Beta: '\n",
    "            print self.beta\n",
    "        elif(self.beta_type == 2):\n",
    "            self.beta = getBetaStochasticGradient(self.train_x.values, self.train_y.values, self.alpha)\n",
    "            print 'Beta: '\n",
    "            print self.beta\n",
    "        else:\n",
    "            print 'Incorrect beta_type! Usage: 0 - closed form solution, 1 - batch gradient descent, 2 - stochastic gradient descent'\n",
    "            \n",
    "    # Predicts the y values of all test points\n",
    "    # Outputs the predicted y values to the text file named \"linear-regression-output_betaType_zScore\" inside \"output\" folder\n",
    "    # Computes MSE\n",
    "    def predict(self):\n",
    "        self.predicted_y = self.test_x.values.dot(self.beta)\n",
    "        np.savetxt('output/linear-regression-output' + '_' + str(self.beta_type) + '_' + str(self.z_score) + '.txt', self.predicted_y)\n",
    "        compute_mse(self.predicted_y, self.test_y.values)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # Change 1st paramter to 0 for closed form, 1 for batch gradient, 2 for stochastic gradient\n",
    "    # Add a second paramter with value 1 for z score normalization\n",
    "    print '------------------------------------------------'\n",
    "    print 'Closed Form Without Normalization'\n",
    "    lm = LinearRegression(0)\n",
    "    lm.predict()\n",
    "    \n",
    "    print '------------------------------------------------'\n",
    "    print 'Batch Gradient Without Normalization'\n",
    "    lm = LinearRegression(1)\n",
    "    lm.predict()\n",
    "    \n",
    "    print '------------------------------------------------'\n",
    "    print 'Stochastic Gradient Without Normalization'\n",
    "    lm = LinearRegression(2)\n",
    "    lm.predict()\n",
    "    \n",
    "    print '------------------------------------------------'\n",
    "    print 'Closed Form With Normalization'\n",
    "    lm = LinearRegression(0, 1)\n",
    "    lm.predict()\n",
    "    \n",
    "    print '------------------------------------------------'\n",
    "    print 'Batch Gradient With Normalization'\n",
    "    lm = LinearRegression(1, 1)\n",
    "    lm.predict()\n",
    "    \n",
    "    print '------------------------------------------------'\n",
    "    print 'Stochastic Gradient With Normalization'\n",
    "    lm = LinearRegression(2, 1)\n",
    "    lm.predict()\n",
    "    print '------------------------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
