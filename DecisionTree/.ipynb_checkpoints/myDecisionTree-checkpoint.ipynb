{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '/Users/qiaoyang/Library/Jupyter/runtime/kernel-f7d25a01-be16-4769-986e-d504e6783fda.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bdeae251bcff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m27145\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0msel_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0msel_func\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '/Users/qiaoyang/Library/Jupyter/runtime/kernel-f7d25a01-be16-4769-986e-d504e6783fda.json'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter\n",
    "\n",
    "# Class for instances with operations\n",
    "class Instances(object):\n",
    "    def __init__(self):\n",
    "        self.label = []\n",
    "        self.attrs = []\n",
    "        self.num_attrs = -1\n",
    "        self.num_instances = 0\n",
    "        self.attr_set = []\n",
    "        \n",
    "        #self.label_set = []\n",
    "        \n",
    "\n",
    "    def add_instance(self, _lbl, _attrs):\n",
    "        self.label.append(_lbl)\n",
    "        self.attrs.append(_attrs)\n",
    "        if self.num_attrs == -1:\n",
    "            self.num_attrs = len(_attrs)\n",
    "        else:\n",
    "            assert(self.num_attrs == len(_attrs))\n",
    "        self.num_instances += 1\n",
    "        assert(self.num_instances == len(self.label))\n",
    "\n",
    "    \n",
    "    def make_attr_set(self):\n",
    "        self.attr_set = [set([self.attrs[i][j] for i in range(self.num_instances)]) for j in range(self.num_attrs)]\n",
    "        \n",
    "    #def make_label_set(self):\n",
    "        #self.label_set = set([self.label[i] for i in range(self.num_instances)])\n",
    "\n",
    "\n",
    "    def load_file(self, file_name):\n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                data = line.strip().split(',')\n",
    "                self.add_instance(data[0], data[1:])\n",
    "        self.make_attr_set()\n",
    "        #self.make_label_set()\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def split(self, att_idx):\n",
    "        assert(0 <= att_idx < self.num_attrs)\n",
    "        split_data = {x: Instances() for x in self.attr_set[att_idx]}\n",
    "        for i in range(self.num_instances):\n",
    "            key = self.attrs[i][att_idx] # ith data's att_index's value\n",
    "            split_data[key].add_instance(self.label[i], self.attrs[i])\n",
    "        for key in split_data:\n",
    "            split_data[key].attr_set = self.attr_set\n",
    "            #split_data[key].label_set= self.label_set\n",
    "        return split_data\n",
    "    \n",
    "\n",
    "    def shuffle(self):\n",
    "        indices = list(range(len(self.label)))\n",
    "        random.shuffle(indices)  #shuffle a 3(from 1 to 3) value array --> get array [3,1,2]/or other order radomly\n",
    "        res = Instances()\n",
    "        for x in indices:\n",
    "            res.add_instance(self.label[x], self.attrs[x])\n",
    "        res.attr_set = self.attr_set\n",
    "        #add@\n",
    "        #res.label_set = self.label_set\n",
    "        return res\n",
    "\n",
    "\n",
    "    def get_subset(self, keys):\n",
    "        res = Instances()\n",
    "        for x in keys:\n",
    "            res.add_instance(self.label[x], self.attrs[x])\n",
    "        res.attr_set = self.attr_set\n",
    "        #add@\n",
    "        #res.label_set = self.label_set\n",
    "        return res\n",
    "\n",
    "\n",
    "def compute_entropy(data): #data is a Instances()\n",
    "    total_entropy = 0.0\n",
    "    ########## Please Fill Missing Lines Here ##########\n",
    "    \n",
    "    label_ratio = 0\n",
    "    label_map = {} #label and its count \n",
    "    #create label set (with distinct labels), move to instance class function\n",
    "    label_set = set([data.label[i] for i in range(data.num_instances)]) \n",
    "    #initialization\n",
    "    label_map = {label_key: 0 for label_key in label_set}\n",
    "    for instance_label in data.label:\n",
    "        label_map[instance_label] = label_map[instance_label] + 1\n",
    "    \n",
    "    #calculate label_ratio\n",
    "    for label_index in label_set:\n",
    "        label_ratio = label_map[label_index] / data.num_instances\n",
    "        if label_ratio != 0:\n",
    "            total_entropy = total_entropy - label_ratio * np.log2(label_ratio)\n",
    "\n",
    "    return total_entropy\n",
    "    \n",
    "\n",
    "def compute_info_gain(data, att_idx): # this data only have the proper data after split\n",
    "    info_gain = 0.0\n",
    "    info = 0.0\n",
    "    #create label set (with distinct labels), move to instance class function\n",
    "    #label_set = set([data.label[i] for i in range(data.num_instances)]) \n",
    "    \n",
    "    attrValues_set = data.attr_set[att_idx] #is a set\n",
    "    \n",
    "    ########## Please Fill Missing Lines Here ##########\n",
    "    attrValue_count_map = {} #attr_value of this attr and how many data has the value\n",
    "    attrValue_count_map = {x: 0 for x in attrValues_set}  #initialization\n",
    "    attrValue_label_map = {} #attr_value and its labels that have this attr_value\n",
    "    attrValue_label_map = {y: [] for y in attrValues_set}  #initialization\n",
    "    #get attrValue_count_map, and attrValue_label_map, traverse all the data\n",
    "    for dataIndex in range(0, data.num_instances):\n",
    "        #attr_value = data.attrs[dataIndex,att_idx]\n",
    "        attr_value = (data.attrs[dataIndex])[att_idx]\n",
    "        attrValue_count_map[attr_value] =  attrValue_count_map[attr_value] + 1\n",
    "        attrValue_label_map[attr_value].append(data.label[dataIndex])\n",
    "\n",
    "    #for attr_value in self.attrs[:,att_idx]:\n",
    "    #    attrValue_count_map[attr_value] =  attrValue_count_map[attr_value] + 1\n",
    "    #calculate attr_value_ratio\n",
    "   \n",
    "    for value_temp in attrValues_set:\n",
    "        label_count_map = {} #label number in this value: key-label; value-number of this label for this attrValue\n",
    "        attrValue_ratio = attrValue_count_map[value_temp] / data.num_instances\n",
    "        #get label ratio in this attriValue\n",
    "        #initialization\n",
    "        #for label_temp in attrValue_label_map[value_temp]:  #attrValue_label_map[value_temp] is a array that contain the labels\n",
    "            #label_count_map[label_temp] = 0\n",
    "            #label_count_map = {label_temp: 0}\n",
    "        label_count_map = {label_temp: 0 for label_temp in attrValue_label_map[value_temp]}\n",
    "        #print ('attrValue_label_map[value_temp]')\n",
    "        #print (attrValue_label_map[value_temp])\n",
    "        #print ('label_count_map')\n",
    "        #print (label_count_map)\n",
    "        for label_temp1 in attrValue_label_map[value_temp]:\n",
    "            #print('label_temp1')\n",
    "            #print(label_temp1)\n",
    "            label_count_map[label_temp1] = label_count_map[label_temp1] + 1\n",
    "        \n",
    "        temp_entropy = 0 #temp_entropy is I(2,3) in formula\n",
    "        for label_key in label_count_map:\n",
    "            ratio = label_count_map[label_key] / len(attrValue_label_map[value_temp])\n",
    "            if ratio != 0:\n",
    "                temp_entropy = temp_entropy - ratio * np.log2(ratio)\n",
    "            \n",
    "        info = info + attrValue_ratio * temp_entropy\n",
    "      \n",
    "    info_gain = compute_entropy(data) - info\n",
    "    \n",
    "    return info_gain\n",
    "\n",
    "\n",
    "def comput_gain_ratio(data, att_idx):\n",
    "    gain_ratio = 0.0\n",
    "    splitInfo = 0.0\n",
    "    ########## Please Fill Missing Lines Here ##########    \n",
    "    attrValues_set = data.attr_set[att_idx] #is a set\n",
    "    \n",
    "    ########## Please Fill Missing Lines Here ##########\n",
    "    attrValue_count_map = {} #attr_value of this attr and how many data has the value\n",
    "    attrValue_count_map = {x: 0 for x in attrValues_set}  #initialization\n",
    "    \n",
    "    for dataIndex in range(0, data.num_instances):\n",
    "        #attr_value = data.attrs[dataIndex,att_idx]\n",
    "        attr_value = (data.attrs[dataIndex])[att_idx]\n",
    "        attrValue_count_map[attr_value] =  attrValue_count_map[attr_value] + 1\n",
    "    \n",
    "    for value_temp in attrValues_set:\n",
    "        attrValue_ratio = attrValue_count_map[value_temp] / data.num_instances\n",
    "        if attrValue_ratio != 0:\n",
    "            splitInfo = splitInfo - attrValue_ratio * np.log2(attrValue_ratio)\n",
    "    \n",
    "    if splitInfo != 0:  #if == 0, it's a leaf node\n",
    "        gain_ratio = compute_info_gain(data, att_idx) / splitInfo\n",
    "    \n",
    "    return gain_ratio\n",
    "\n",
    "\n",
    "# Class of the decision tree model based on the ID3 algorithm\n",
    "class DecisionTree(object): #object means send in many argument: from __init__ we know, object contain _instances and _sel_func\n",
    "    def __init__(self, _instances, _sel_func):\n",
    "        self.instances = _instances\n",
    "        self.sel_func = _sel_func\n",
    "        self.gain_function = compute_info_gain if _sel_func == 0 else comput_gain_ratio\n",
    "        self.m_attr_idx = None # The decision attribute if the node is a branch\n",
    "        self.m_class = None # The decision class if the node is a leaf\n",
    "        self.make_tree()\n",
    "\n",
    "    def make_tree(self): #self is the split_data's instances\n",
    "        if self.instances.num_instances == 0:\n",
    "            # No any instance for this node\n",
    "            self.m_class = '**MISSING**'\n",
    "        else:\n",
    "            gains = [self.gain_function(self.instances, i) for i in range(self.instances.num_attrs)]\n",
    "            #print ('gains')\n",
    "            #print (gains)\n",
    "            self.m_attr_idx = np.argmax(gains) #Returns the indices of the maximum value. select new sttribute\n",
    "            if np.abs(gains[self.m_attr_idx]) < 1e-9:\n",
    "                # A leaf to decide the decided class\n",
    "                self.m_attr_idx = None\n",
    "                ########## Please Fill Missing Lines Here ##########\n",
    "                #now, all the instances have the same label\n",
    "                self.m_class = self.instances.label[0]\n",
    "            else:\n",
    "                # A branch\n",
    "                split_data = self.instances.split(self.m_attr_idx) # put all data in map, classify by value of this attr\n",
    "                self.m_successors = {x: DecisionTree(split_data[x], self.sel_func) for x in split_data}\n",
    "                for x in self.m_successors:\n",
    "                    self.m_successors[x].make_tree()\n",
    "\n",
    "    def classify(self, attrs):\n",
    "        assert((self.m_attr_idx != None) or (self.m_class != None))\n",
    "        if self.m_attr_idx == None:\n",
    "            return self.m_class\n",
    "        else:\n",
    "            return self.m_successors[attrs[self.m_attr_idx]].classify(attrs)\n",
    "            \n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) < 1 + 1:\n",
    "        print('--usage python3 %s data [0/1, 0-Information Gain, 1-Gain Ratio, default: 0]' % sys.argv[0], file=sys.stderr)\n",
    "        sys.exit(0)\n",
    "    random.seed(27145)\n",
    "    np.random.seed(27145)\n",
    "    \n",
    "    sel_func = int(sys.argv[2]) if len(sys.argv) > 1 + 1 else 0\n",
    "    assert(0 <= sel_func <= 1) \n",
    "\n",
    "    data = Instances().load_file(sys.argv[1])\n",
    "    data = data.shuffle()\n",
    "    print (data)\n",
    "    print ('data')\n",
    "    \n",
    "    # 5-Fold CV\n",
    "    kf = KFold(n_splits=5)\n",
    "    n_fold = 0\n",
    "    accuracy = []\n",
    "    for train_keys, test_keys in kf.split(range(data.num_instances)):\n",
    "        train_data = data.get_subset(train_keys)\n",
    "        test_data = data.get_subset(test_keys)\n",
    "        n_fold += 1\n",
    "        model = DecisionTree(train_data, sel_func)\n",
    "        predictions = [model.classify(test_data.attrs[i]) for i in range(test_data.num_instances)]\n",
    "        num_correct_predictions = sum([1 if predictions[i] == test_data.label[i] else 0 for i in range(test_data.num_instances)])\n",
    "        nfold_acc = float(num_correct_predictions) / float(test_data.num_instances)\n",
    "        accuracy.append(nfold_acc)\n",
    "        print('Fold-{}: {}'.format(n_fold, nfold_acc))\n",
    "\n",
    "    print('5-CV Accuracy = {}'.format(np.mean(accuracy)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
